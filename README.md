# container-llm-server
Repository for self try about running LLM on container using llama.cpp
